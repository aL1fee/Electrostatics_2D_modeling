// for release:
// int WinMain(HINSTANCE hThisInstance, HINSTANCE hPrevInstance, LPSTR lpszArgument, int nCmdShow)
// Linker -> Linker, system, windows
//
//
// === Physics sources for sims
Caps & dielectrics: https://ocw.mit.edu/courses/8-02-physics-ii-electricity-and-magnetism-spring-2007/ba7ba4fb31a34a167dc98bf25904d38a_chap5capacitance.pdf
the whole book: https://ocw.mit.edu/courses/8-02-physics-ii-electricity-and-magnetism-spring-2007/pages/readings/


//
// === C++ ===
The statement int* x = 5; in C++ is problematic. Here’s why:
Pointer Initialization: int* x declares x as a pointer to an integer.
Assignment: = 5 attempts to assign the value 5 to the pointer x.
However, this is incorrect because 5 is not a valid memory address. 
Pointers should be assigned either the address of a variable (using the & operator) 
or a dynamically allocated memory address (using new or malloc).

Freeing up memory:
Prevent Memory Leaks: If you don’t free memory that you’ve allocated, it remains occupied even if it’s no longer needed.
Over time, this can lead to memory leaks, where the available memory is gradually consumed, 
potentially causing your program to crash or slow down.
	int* ptr = new int(10); // Allocate memory
	// Use the memory
	delete ptr; // Free the memory

Time test:
	auto testExecutionRenderLoopTime0 = std::chrono::high_resolution_clock::now();
	auto testExecutionRenderLoopTime1 = std::chrono::high_resolution_clock::now();
	auto duration = std::chrono::duration_cast<std::chrono::microseconds> (testExecutionRenderLoopTime1 - testExecutionRenderLoopTime0);
	std::cout << "Time test: " << duration.count() << std::endl;







//
//
//
//
//
//
// ==Earlier OpenGL notes==
// OpenGL operates as a state 
// A triangle contains the smallest number of verticies to represent one flat plane with a normal that's pointing in the same direction.
// Because of that, GPUs tend to use triangles as their rendering primitive.
// Rendering is the process by which a computer creates an image from models.
// 
// A shader is a program that runs on a GPU.  OpenGL includes all the compiler tools internally to take the source code of your shader and create the code that the GPU needs to execute.
// It runs on a GPU because it is much faster.
// Like everything in openGL, shaders work as state machines.
// NOTE: shaders are usually called a lot of time for each vertex/pixel --> A lot of room for optimization!
// Although there are several shaders, vertex and fragment shaders usually make up about 80-90% of a typical graphics application.
// Vertex shaders. They get called for each vertex we want to render. The primary purpose of a vertex shader is to tell openGL where you want that vertex to be in the screenspace.
//		It is also used to pass data from attributes into the next stage. 
// Fragment (pixel) shaders. They get called for each pixel that needs to be rastarized (drawn on the screen - in the case of a triangle, the pixels between the verticies
//		get filled in with a specific color so that the end user sees a clear triangle). The primary purpose of the fragment shader is to decide what color each pixel is supposed to be.
// In the graphics pipeline, after the CPU makes certain calls, GPU takes the lead and runs a series of shader programs before we see the result rendered. 
// A pixel is the smallest visible element on your display.The pixels in your system are stored
//		in a framebuffer, which is a chunk of memory that the graphics hardware manages and feeds to your display device.
// Attributes – Global variables that may change per vertex, that are passed from the OpenGL application to vertex shaders
// 
// There are vertex buffers (buffers that store vertices), index (element) buffers (ones that store indices of the vertices).
// There are vertex arrays (an openGL special - doesn't exist in other rendering APIs). They are a way to bind vertex buffers with a certain specification/layout
//		of those vertex buffers. Using vertex arrays helps to make the code easier when having lots of objects to render, by not requiring the programmer
//		to explicitly specify the layout of a vertex buffer using glVertexAttribPointer command.
//		Vertex array object contains a binding between vertex buffers and their respective specifications/layouts.
//		Vertex array objects are mandatory. OpenGL compatibility profile creates VAOs by default. The core profile doesn't -- the programer has to do that.
//		glVertexAttribPointer binds the vertex array with the currently bound vertex buffer.
// Using a single VAO is considered faster, but is likely less convenient. OpenGL recommends using VAOs.
// 
// 
// Uniforms are a way to get data from the CPU to the shaders (e.g. changing the color in the fragment shader dynamically rather than hardcoding it)
// Uniforms are set per draw call (while attributes are set per vertex)
// 
// Rendering pipeline -  a sequence of processing stages for converting the data your application provides to OpenGL into a final rendered image
//		1. Vertex Data
//			OpenGL requires that all data be stored in buffer objects, which are just chunks of memory managed by OpenGL
//			Drawing in OpenGL usually means transferring vertex data to the OpenGL server.Think of a vertex as a bundle of data values that are processed together.
//		2. Vertex Shader
//			For each vertex that is issued by a drawing command, a vertex shader will be called to process the data associated with that vertex.
//		3. Tesselation Control Shader
//			tessellation uses patches to describe an object’s shape and allows relatively simple collections of patch geometry to be tessellated
//			to increase the number of geometric primitives, providing better - looking models
//		4. Tesselation Evaluation Shader
//			The tessellation shading stage can potentially use two shaders to manipulate the patch data and generate the final shape
//		5. Geometry Shader
//			Allows additional processing of individual geometric primitives, including creating new ones, before rasterization
//		6. Primitive Setup
//			Organizes the vertices into their associated geometric primitives in preparation for clipping and rasterization
//		7. Culling and Clipping
//			Only the primitives which are within the visual volume need to actually be rastered (drawn).Primitives which are completely outside the visual volume are discarded.
//		8. Rasterization
//			Determines which screen locations are covered by a particular piece of geometry (point, line, or triangle).Knowing those locations, 
//			along with the input vertex data, the rasterizer linearly interpolates the data values for each varying variable in the 
//			fragment shader and sends those values as inputs into your fragment shader. Consider a fragment a “candidate pixel,” in that pixels
//			have a home in the framebuffer, while a fragment still can be rejected and never update its associated pixel location.
//		9. Fragment Shader
//			Determines the fragment’s final color(although the next stage, per - fragment operations, can modify the color one last time) 
//			and potentially its depth value.  A fragment shader may also terminate processing a fragment if it determines the fragment
//			shouldn’t be drawn; this process is called fragment discard
//		If a fragment successfully makes it through all of the enabled tests, it may be written directly to the framebuffer, 
//		updating the color(and possibly depth value) of its pixel, or if blending is enabled, the fragment’s color will be
//		combined with the pixel’s current color to generate a new color that is written into the framebuffer.
//
// Abstaction:
//		1. Vertex buffers
//		2. Index (element) buffers
//		3. Vertex array
//		4. Render function
//		5. Shaders
//
// A material is a sjader w/ a set of data (a shader with uniforms)
// 
// A texture map is an image applied (mapped) to the surface of a shape or polygon.
//		1. We load an image
//		2. We create a texture in OpenGL
//		3. We bind that texture	when it's time to render
//		4. We modify our shader to bind to that texture slot
//		5. We sample that texture in our shader
//		6. We see the texture when we load the shader
//
// The bottom left in OpenGL is (0,0). OpenGL expects our textures to start in the bottom left. Typically, when an image is loaded, it's stored from the top left to the bottom right.
//

	Double buffer. When an application draws in a single buffer the resulting image may
display flickering issues. This is because the resulting output image is not drawn in an
instant, but drawn pixel by pixel and usually from left to right and top to bottom. Because
this image is not displayed at an instant to the user while still being rendered to, the result
may contain artifacts. The front buffer contains the final output image that is shown at
the screen, while all the rendering commands draw to the back buffer. As soon as all
the rendering commands are finished we swap the back buffer to the front buffer so the
image can be displayed without still being rendered to, removing all the aforementioned
artifacts.

	A frame - am iteration of the render loop.

	The process of transforming 3D coordinates to 2D pixels is managed by the graphics pipeline of OpenGL.
The graphics pipeline can be divided into two large parts: the first transforms your 3D coordinates
into 2D coordinates and the second part transforms the 2D coordinates into actual colored pixels.

	The graphics pipeline takes as input a set of 3D coordinates and transforms these to colored
2D pixels on your screen. The graphics pipeline can be divided into several steps where each step
requires the output of the previous step as its input. All of these steps are highly specialized (they
have one specific function) and can easily be executed in parallel. Because of their parallel nature,
graphics cards of today have thousands of small processing cores to quickly process your data within
the graphics pipeline. The processing cores run small programs on the GPU for each step of the
pipeline. These small programs are called shaders.

	Shaders run on the GPU, saving us valuable CPU time. Shaders are written in the OpenGL Shading Language (GLSL).

	Graphics pipeline:
		0. Vertex Data[]
			OpenGL requires that all data be stored in buffer objects, which are just chunks of memory managed by OpenGL.
			Drawing in OpenGL usually means transferring vertex data to the OpenGL server.
			Think of a vertex as a bundle of data values that are processed together.
		*1. Vertex Shader (runs once per vertex) [*Required to be defined]
			For each vertex that is issued by a drawing command, a vertex shader will be called to process the data associated with that vertex.
		2. Shape assembly (Tessalation Control and Evaluation Shader)
			Takes as input all the vertices from the vertex shader that form a primitive and assembles all the point(s) in the primitive shape given.
		3. Geometry shader
			Allows additional processing of individual geometric primitives, including creating new ones, before rasterization.
		4. Tests and blending (+Culling and Clipping)
			This stage checks the corresponding depth (and stencil) value (we’ll get to those later) of the fragment and uses those to 
			check if the resulting fragment is in front or behind other objects and should be discarded accordingly. The stage also checks for 
			alpha values (alpha values define the opacity of an object) and blends the objects accordingly.
			Only the primitives which are within the visual volume need to actually be rastered (drawn).Primitives which are completely 
			outside the visual volume are discarded.
		*5. Fragment shader (runs once per pixel) [*Required to be defined]
			The main purpose of the fragment shader is to calculate the final color of a pixel and this is 
			usually the stage where all the advanced OpenGL effects occur. Usually the fragment shader contains 
			data about the 3D scene that it can use to calculate the final pixel color (like lights, shadows, color of the light and so on)
		6. Rasterization
			Determines which screen locations are covered by a particular piece of geometry (point, line, or triangle).Knowing those locations, 
			along with the input vertex data, the rasterizer linearly interpolates the data values for each varying variable in the fragment 
			shader and sends those values as inputs into your fragment shader. Consider a fragment a “candidate pixel,” in that pixels have 
			a home in the framebuffer, while a fragment still can be rejected and never update its associated pixel location.

	As input to the graphics pipeline we pass in a list of three 3D coordinates that should form a
triangle in an array here called Vertex Data; this vertex data is a collection of vertices. A vertex
is a collection of data per 3D coordinate. This vertex’s data is represented using vertex attributes
that can contain any data we’d like.

	OpenGL only processes 3D coordinates when they're in a specific range between -1.0 and 1.0 on all 3 axes (x, y, and z). All coordinates
within this so called normalized device coordinates range will end up visible on your screen (and all coordinates outside this region won’t).

	Your NDC coordinates will then be transformed to screen-space coordinates via the
viewport transform using the data you provided with glViewport. The resulting
screen-space coordinates are then transformed to fragments as inputs to your fragment
shader.

	Vertex data is sent as input to the first process of the graphics pipeline: the vertex shader.
This is done by creating memory on the GPU where we store the vertex data, configure how OpenGL should 
interpret the memory and specify how to send the data to the graphics card. The vertex shader then 
processes as much vertices as we tell it to from its memory.

	Vertex Buffer Objects (VBO) can store a large number of vertices in the GPU's memory.
Sending data to the graphics card from the CPU is relatively slow, so wherever we can we try to send
as much data as possible at once. Once the data is in the graphics card’s memory the vertex shader has
almost instant access to the vertices making it extremely fast.
	VBO has a unique ID corresponding to that buffer.
	OpenGL has many types of buffer objects and the buffer type of a vertex buffer object is GL_ARRAY_BUFFER. 

	OpenGL allows us to bind to several buffers at once as long as they have a
different buffer type. Once a certain buffer is bound, any buffer calls one makes 
will be used to configure the currently bound buffer.

	GLSL is the OpenGL Shading Language.
		Each shader begins with a declaration of its version.
		Next we declare all the input vertex attributes in the vertex shader with the in keyword.

	A vector in GLSL has a max size of 4 and each of its values can be retrieved via vec.x, vec.y, vec.z and vec.w.
vec.w is used for something called perspective division.

	Colors in computer graphics are represented as an array of 4 values: the red, green, blue
and alpha (opacity) component, commonly abbreviated to RGBA. When defining a color
in OpenGL or GLSL we set the strength of each component to a value between 0.0 and
1.0.

	A shader program object is the final linked version of multiple shaders combined. To use the recently
compiled shaders we have to link them to a shader program object and then activate this shader
program when rendering objects. The activated shader program’s shaders will be used when we
issue render calls.
	When linking the shaders into a program it links the outputs of each shader to the inputs of the
next shader. You'll get linking errors if your outputs and inputs do not match

	OpenGL needs to be told how it should interpret the vertex data (per vertex attribute) using
glVertexAttribPointer call.

	Each vertex attribute takes its data from memory managed by a VBO and which VBO it
takes its data from (you can have multiple VBOs) is determined by the VBO currently
bound to GL_ARRAY_BUFFER when calling glVertexAttribPointer. Since
the previously defined VBO is still bound before calling glVertexAttribPointer
vertex attribute 0 is now associated with its vertex data.
	Vertex attributes are disabled by default.

	A vertex array object (VAO) can be bound just like a vertex buffer object and any
subsequent vertex attribute calls from that point on will be stored inside the VAO. This has the
advantage that when configuring vertex attribute pointers you only have to make those calls once
and whenever we want to draw the object, we can just bind the corresponding VAO. This makes
switching between different vertex data and attribute configurations as easy as binding a different
VAO. All the state we just set is stored inside the VAO.
	Vertex array object stores the following:
		1. Calls to  glEnableVertexAttribArray or glDisableVertexAttribArray
		2. Vertex attribute configurations via glVertexAttribPointer
		3. Vertex buffer objects associated with vertex attributes by calls to glVertexAttribPointer.

	Core OpenGL requires that we use a VAO so it knows what to do with our vertex inputs.

	Element (index) buffer objects (EBO) is a buffer that stores indices that OpenGL uses to decide what 
vertices to draw. That is called indexed drawing. We have to specify the unique vertices and the indices to
draw them as a rectangle.

	The glDrawElements function takes its indices from the EBO currently bound to the
GL_ELEMENT_ARRAY_BUFFER target. This means we have to bind the corresponding EBO
each time we want to render an object with indices which again is a bit cumbersome. It just so
happens that a vertex array object also keeps track of element buffer object bindings. The last
element buffer object that gets bound while a VAO is bound, is stored as the VAO’s element buffer
object. Binding to a VAO then also automatically binds that EBO.
	A VAO stores the glBindBuffer calls when the target is
GL_ELEMENT_ARRAY_BUFFER. This also means it stores its unbind calls so
make sure you don’t unbind the element array buffer before unbinding your VAO,
otherwise it doesn’t have an EBO configured.

	Shaders are little programs that rest on the GPU. The only communication they have is
via their inputs and outputs.
	GLSL is tailored for use with graphics and contains
useful features specifically targeted at vector and matrix manipulation.
	Shaders always begin with a version declaration, followed by a list of input and output variables,
uniforms and its main function. Each shader’s entry point is at its main function where we process
any input variables and output the results in its output variables.
	A vertex attribute is every input variable to the vertex shader.
	A vector in GLSL is a 1,2,3 or 4 component container for any of the basic types (int, flaot, double, uint, bool).
	Vectors are a flexible datatype that we can use for all kinds of input and output.
	Swizzling - you can use x, y, z, or w, referring to the first, second, third, and fourth components, respectively.
	Vertex shader receives its input straight from the vertex data. To define how the 
vertex data is organized we specify the input variables with location metadata so we
can configure the vertex attributes on the CPU. This is often done as layout
(location = 0). The vertex shader thus requires an extra layout specification for its inputs so
we can link it with the vertex data.
	The other exception is that the fragment shader requires a vec4 color output variable, since the
fragment shaders needs to generate a final output color. If you fail to specify an output color in your
fragment shader, the color buffer output for those fragments will be undefined (which usually means
OpenGL will render them either black or white).
	So if we want to send data from one shader to the other we’d have to declare an output in the
sending shader and a similar input in the receiving shader. When the types and the names are equal
on both sides OpenGL will link those variables together and then it is possible to send data between
shaders (this is done when linking a program object).

	Uniforms are a way to pass data from our application on the CPU to the shaders on the GPU.
	Uniforms are global. Global, meaning that a uniform variable is unique per shader program object, and can be
accessed from any shader at any stage in the shader program
	Whatever you set the uniform value to, uniforms will keep their values until they’re either reset or updated.
		!!!
		!!!Note that finding the uniform location does not require you to use the shader program first, but updating a
uniform does require you to first use the program (by calling glUseProgram), because it sets the
uniform on the currently active shader program.

	Fragment interpolation in the fragment shader. When rendering a triangle the rasterization stage usually
results in a lot more fragments than vertices originally specified. The rasterizer then determines the
positions of each of those fragments based on where they reside on the triangle shape. Based on
these positions, it interpolates all the fragment shader’s input variables.

	A texture is a 2D image (even 1D and 3D textures exist) used to add detail to an object; think of a texture as a piece of paper with a
nice brick image (for example) on it neatly folded over your 3D house so it looks like your house
has a stone exterior. Because we can insert a lot of detail in a single image, we can give the illusion
the object is extremely detailed without having to specify extra vertices.
	In order to map a texture to the triangle we need to tell each vertex of the triangle which part of
the texture it corresponds to. Each vertex should thus have a texture coordinate associated with them
that specifies what part of the texture image to sample from. Fragment interpolation then does the
rest for the other fragments.
	Texture coordinates range from 0 to 1 in the x and y axis (remember that we use 2D texture im￾ages).
Retrieving the texture color using texture coordinates is called sampling. Texture coordinates
start at (0,0) for the lower left corner of a texture image to (1,1) for the upper right corner of a
texture image.
	Texture sampling has a loose interpretation and can be done in many different ways. It is thus
our job to tell OpenGL how it should sample its textures.

	Texture coordinates do not depend on resolution but can be any floating point value, thus OpenGL
has to figure out which texture pixel (also known as a texel ) to map the texture coordinate to. This
becomes especially important if you have a very large object and a low resolution texture. 
OpenGL has options for this texture filtering, two important ones: GL_NEAREST, GL_LINEAR.
	When set to GL_NEAREST, OpenGL selects the texel that center is closest
to the texture coordinate.
	GL_LINEAR (also known as (bi)linear filtering) takes an interpolated value from the texture
coordinate’s neighboring texels, approximating a color between the texels.
	Texture filtering can be set for magnifying and minifying operations (when scaling up or
downwards) so you could for example use nearest neighbor filtering when textures are scaled
downwards and linear filtering for upscaled textures. We thus have to specify the filtering method
for both options via glTexParameter*.

	Mipmaps is a collection of texture images where each subsequent texture is twice as small compared
to the previous one. The idea behind mipmaps should be easy to understand: after a certain distance threshold from the viewer,
OpenGL will use a different mipmap texture that best suits the distance to the object. Because the
object is far away, the smaller resolution will not be noticeable to the user. OpenGL is then able to
sample the correct texels, and there’s less cache memory involved when sampling that part of the
mipmaps.
	Creating a collection of mipmapped textures for each texture image is cumbersome to do manu￾ally, but luckily OpenGL is able 
to do all the work for us with a single call to glGenerateMipmaps after we’ve created a texture.

	The first thing we need to do to actually use textures is to load them into our application. We need to convert the image 
format into a large array of bytes. The stb_image.h image-loading library supports
several popular formats and does all the hard work.

	The fragment shader should also have access to the texture object, but how do we pass the
texture object to the fragment shader? GLSL has a built-in data-type for texture objects called
a sampler that takes as a postfix the texture type we want e.g. sampler1D, sampler3D or in
our case sampler2D. We can then add a texture to the fragment shader by simply declaring a
uniform sampler2D that we later assign our texture to.S
	To sample the color of a texture we use GLSL’s built-in texture function that takes as its
first argument a texture sampler and as its second argument the corresponding texture coordinates.
The texture function then samples the corresponding color value using the texture parameters
we set earlier. The output of this fragment shader is then the (filtered) color of the texture at the
(interpolated) texture coordinate.
	 Using glUniform1i we can actually assign a location value to
the texture sampler so we can set multiple textures at once in a fragment shader. This location of
a texture is more commonly known as a texture unit. The default texture unit for a texture is 0
which is the default active texture unit.
	The main purpose of texture units is to allow us to use more than 1 texture in our shaders. By
assigning texture units to the samplers, we can bind to multiple textures at once as long as we
activate the corresponding texture unit first. Just like glBindTexture we can activate texture
units using glActiveTexture passing in the texture unit we’d like to use:
		glActiveTexture(GL_TEXTURE0); // activate texture unit first
		glBindTexture(GL_TEXTURE_2D, texture);
	OpenGL should have a at least a minimum of 16 texture units for you to use which you
can activate using GL_TEXTURE0 to GL_TEXTURE15.
	We have to tell OpenGL to which texture unit each shader sampler belongs to by setting
each sampler using glUniform1i. We only have to set this once, so we can do this before we
enter the render loop. Don’t forget to activate the shader first!

Transformations:
	Dot product: v . k = |v| |k| cos(theta)
		(allows to easily test if the two vectors are orthogonal or parallel to each other)
	Cross product: a * b = |a| |b| sin(theta) n, where n is a unit vector || to a & b.
		(takes two non-parallel vecs as input and produces a third vector that's orthogonal to both input vecs)
	A matrix is a rectangular array of numbers, symbols and/or mathematical expressions. 
		Each individual item in a matrix is called an element of the matrix. 
		Matrices are indexed by (i,j) where i is the row and j is the column.
	Matrix multiplication is a combination of normal multiplication and addition using 
		the left-matrix’s rows with the right￾matrix’s columns. 
	A vector is basically a Nx1 matrix where N is the vector’s number of components (also known as an
		N-dimensional vector).
	If we have a MxN matrix we can multiply this matrix with our Nx1 vector, since the columns
		of the matrix are equal to the number of rows of the vector, thus matrix multiplication is defined.
		It just so happens that there are lots of interesting 2D/3D transformations we can place inside a matrix, 
		and multiplying that matrix with a vector then transforms that vector
	In OpenGL we usually work with 4x4 transformation matrices for several reasons and one of them
		is that most of the vectors are of size 4. The most simple transformation matrix that we can think of
		is the identity matrix. The identity matrix is an NxN matrix with only 0s except on its diagonal.
		This transformation matrix leaves a vector completely unharmed.
	If a scalar is equal on all axes, it is a uniform scaling. Otherwise it's non-uniform scaling.
	Translation is the process of adding another vector on top of the original vector to return a new
		vector with a different position, thus moving the vector based on a translation vector. W
	Homogeneous coordinates The w component of a vector is also known as a homoge￾neous coordinate. To get the 3D vector from a homogeneous vector we divide the x, y and
		z coordinate by its w coordinate. We usually do not notice this since the w component is
		1.0 most of the time. Using homogeneous coordinates has several advantages: it allows
		us to do matrix translations on 3D vectors (without a w component we can’t translate
		vectors) and in the next chapter we’ll use the w value to create 3D perspective. Also,
		whenever the homogeneous coordinate is equal to 0, the vector is specifically known as a
		direction vector since a vector with a w coordinate of 0 cannot be translated.
	A rotation in 2D or 3D is represented with an angle. An angle could be in degrees or radians where a whole circle has 360 degrees or 2 PI2
		radians.
	Rotations in 3D are specified with an angle and a rotation axis. The angle specified will rotate
		the object along the rotation axis given. Try to visualize this by spinning your head a certain degree
		while continually looking down a single rotation axis. When rotating 2D vectors in a 3D world for
		example, we set the rotation axis to the z-axis (try to visualize this).
	Using trigonometry it is possible to transform vectors to newly rotated vectors given an angle.
		This is usually done via a smart combination of the sine and cosine functions (commonly
		abbreviated to sin and cos).
	A rotation matrix is defined for each unit axis in 3D space where the angle is represented as the
		theta symbol θ.
	Using the rotation matrices we can transform our position vectors around one of the three unit
		axes. To rotate around an arbitrary 3D axis we can combine all 3 them by first rotating around the
		X-axis, then Y and then Z for example. However, this quickly introduces a problem called Gimbal
		lock. We won’t discuss the details, but a better solution is to rotate around an arbitrary unit axis
		e.g. (0.662,0.2,0.722) (note that this is a unit vector) right away instead of combining the
		rotation matrices. Such a (verbose) matrix exists and is given below with (Rx,Ry,Rz) as the arbitrary
		rotation axis
	The true power from using matrices for transformations is that we can combine multiple transfor￾mations in a single matrix thanks to matrix-matrix multiplication
	Note that we first do a translation and then a scale transformation when multiplying matrices.
		Matrix multiplication is not commutative, which means their order is important. When multiplying
		matrices the right-most matrix is first multiplied with the vector so you should read the multiplications
		from right to left. It is advised to first do scaling operations, then rotations and lastly translations
		when combining matrices otherwise they may (negatively) affect each other

There is an easy-to-use and tailored-for-OpenGL mathematics library called GLM.
GLM stands for OpenGL Mathematics and is a header-only library, which means that we only have
	to include the proper header files and we’re done; no linking and compiling necessary. G

	We can define an infinite amount of transformations and combine them all in a single matrix
that we can re-use as often as we’d like. Using transformations like this in the vertex shader saves
us the effort of re-defining the vertex data and saves us some processing time as well, since we
don’t have to re-send our data all the time (which is quite slow); all we need to do is update the
transformation uniform.

In the last chapter we learned how we can use matrices to our advantage by transforming all vertices
	with transformation matrices. OpenGL expects all the vertices, that we want to become visible, to
	be in normalized device coordinates after each vertex shader run. That is, the x, y and z coordinates
	of each vertex should be between -1.0 and 1.0; coordinates outside this range will not be visible.
	What we usually do, is specify the coordinates in a range (or space) we determine ourselves and in
	the vertex shader transform these coordinates to normalized device coordinates (NDC). These NDC
	are then given to the rasterizer to transform them to 2D coordinates/pixels on your screen.
Transforming coordinates to NDC is usually accomplished in a step-by-step fashion where we
	transform an object’s vertices to several coordinate systems before finally transforming them to
	NDC. The advantage of transforming them to several intermediate coordinate systems is that some
	operations/calculations are easier in certain coordinate systems as will soon become apparent. There
	are a total of 5 different coordinate systems that are of importance to us:
		1. Local space (or Object space)
			Local coordinates are the coordinates of your object relative to its local origin; they’re the
			coordinates your object begins in.
		2. World space
			The next step is to transform the local coordinates to world-space coordinates which are
			coordinates in respect of a larger world. These coordinates are relative to some global origin
			of the world, together with many other objects also placed relative to this world’s origin.
			The coordinates of your object are transformed from local to world space; this is accomplished with the model matrix.
		3. View space (or eye space)
			Next we transform the world coordinates to view-space coordinates in such a way that each
			coordinate is as seen from the camera or viewer’s point of view.
			These combined transformations are generally stored inside a view matrix that
			transforms world coordinates to view space.
		4. Clip space
			After the coordinates are in view space we want to project them to clip coordinates. Clip
			coordinates are processed to the -1.0 and 1.0 range and determine which vertices will end
			up on the screen. Projection to clip-space coordinates can add perspective if using perspective
			projection.
			To transform vertex coordinates from view to clip-space we define a so called projection matrix
			that specifies a range of coordinates e.g. -1000 and 1000 in each dimension. The projection
			matrix then transforms coordinates within this specified range to normalized device coordinates
			(-1.0, 1.0). All coordinates outside this range will not be mapped between -1.0 and 1.0 and
			therefore be clipped. With this range we specified in the projection matrix, a coordinate of (1250,
			500, 750) would not be visible, since the x coordinate is out of range and thus gets converted to a
			coordinate higher than 1.0 in NDC (normalized device coordinates) and is therefore clipped.
			This viewing box a projection matrix creates is called a frustum and each coordinate that ends
			up inside this frustum will end up on the user’s screen. The total process to convert coordinates
			within a specified range to NDC that can easily be mapped to 2D view-space coordinates is called
			projection since the projection matrix projects 3D coordinates to the easy-to-map-to-2D normalized
			device coordinates.
			Once all the vertices are transformed to clip space a final operation called perspective division
			is performed where we divide the x, y and z components of the position vectors by the vector’s
			homogeneous w component; perspective division is what transforms the 4D clip space coordinates
			to 3D normalized device coordinates. This step is performed automatically at the end of the vertex
			shader step.
			It is after this stage where the resulting coordinates are mapped to screen coordinates (using the
			settings of glViewport) and turned into fragments.
			The projection matrix to transform view coordinates to clip coordinates usually takes two differ￾ent forms, where each form defines its own unique frustum. We can either create an orthographic
			projection matrix or a perspective projection matrix.
				1. An orthographic projection matrix defines a cube-like frustum box that defines the clipping space
					where each vertex outside this box is clipped. When creating an orthographic projection matrix we
					specify the width, height and length of the visible frustum. All the coordinates inside this frustum
					will end up within the NDC range after transformed by its matrix and thus won’t be clipped.
					glm::ortho(0.0f, 800.0f, 0.0f, 600.0f, 0.1f, 100.0f);
				2. The projection matrix maps a given frustum range to clip space, but also manipulates the
					w value of each vertex coordinate in such a way that the further away a vertex coordinate is from
					the viewer, the higher this w component becomes. Once the coordinates are transformed to clip
					space they are in the range -w to w (anything outside this range is clipped). OpenGL requires that
					the visible coordinates fall between the range -1.0 and 1.0 as the final vertex shader output, thus
					once the coordinates are in clip space, perspective division is applied to the clip space coordinates
					glm::mat4 proj = glm::perspective(glm::radians(45.0f), (float)width /(float)height, 0.1f, 100.0f);
		5. Screen space
			And lastly we transform the clip coordinates to screen coordinates in a process we call
			viewport transform that transforms the coordinates from -1.0 and 1.0 to the coordinate
			range defined by glViewport. The resulting coordinates are then sent to the rasterizer to
			turn them into fragments.

	Those are all a different state at which our vertices will be transformed in before finally ending
	up as fragments.
	To transform the coordinates from one space to the next coordinate space we’ll use several transfor￾mation matrices of which the most important are the model, view and projection matrix. Our vertex
	coordinates first start in local space as local coordinates and are then further processed to world
	coordinates, view coordinates, clip coordinates and eventually end up as screen coordinates.

	FOV - field of view.

We create a transformation matrix for each of the aforementioned steps: model, view and projection
matrix. A vertex coordinate is then transformed to clip coordinates as follows:

	V_clip = M_proj * M_view * M_model * V_local

	Note that the order of matrix multiplication is reversed (remember that we need to read matrix
	multiplication from right to left). The resulting vertex should then be assigned to gl_Position
	in the vertex shader and OpenGL will then automatically perform perspective division and clipping.

	The output of the vertex shader requires the coordinates to be in clip-space
	which is what we just did with the transformation matrices. OpenGL then performs
	perspective division on the clip-space coordinates to transform them to normalized￾device coordinates. OpenGL then uses the parameters from glViewPort to map the
	normalized-device coordinates to screen coordinates where each coordinate corresponds
	to a point on your screen (in our case a 800x600 screen). This process is called the
	viewport transform.

	By convention, OpenGL is a right-handed system. What this basically says is that
the positive x-axis is to your right, the positive y-axis is up and the positive z-axis is
backwards. Think of your screen being the center of the 3 axes and the positive z-axis
going through your screen towards you. 
Note that in normalized device coordinates OpenGL actually uses a left-handed system (the projection matrix switches
the handedness).

	OpenGL stores all its depth information in a z-buffer, also known as a depth buffer. GLFW
automatically creates such a buffer for you (just like it has a color-buffer that stores the colors of the
output image). The depth is stored within each fragment (as the fragment’s z value) and whenever
the fragment wants to output its color, OpenGL compares its depth values with the z-buffer. If the
current fragment is behind the other fragment it is discarded, otherwise overwritten. This process is
called depth testing and is done automatically by OpenGL.
		However, if we want to make sure OpenGL actually performs the depth testing we first need to
	tell OpenGL we want to enable depth testing; it is disabled by default. We can enable depth testing
	using glEnable. The glEnable and glDisable functions allow us to enable/disable certain
	functionality in OpenGL. That functionality is then enabled/disabled until another call is made to
	disable/enable it. We cam enable depth testing by enabling GL_DEPTH_TEST: glEnable(GL_DEPTH_TEST);
		Since we’re using a depth buffer we also want to clear the depth buffer before each render
	iteration (otherwise the depth information of the previous frame stays in the buffer). Just like clearing
	the color buffer, we can clear the depth buffer by specifying the DEPTH_BUFFER_BIT bit in the
	glClear function: glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);


Camera/View space
	When we’re talking about camera/view space we’re talking about all the vertex coordinates as seen
from the camera’s perspective as the origin of the scene: the view matrix transforms all the world
coordinates into view coordinates that are relative to the camera’s position and direction. To define a
camera we need its position in world space, the direction it’s looking at, a vector pointing to the right
and a vector pointing upwards from the camera.
	The camera position is a vector in world space that points to the camera’s position.
	The next vector required is the camera’s direction e.g. at what direction it is pointing at.
Subtracting the camera position vector from the scene’s origin vector thus results in the direction vector we want.
For the view matrix’s coordinate system we want its z-axis to be positive and because by convention
(in OpenGL) the camera points towards the negative z-axis we want to negate the direction vector. If
we switch the subtraction order around we now get a vector pointing towards the camera’s positive
z-axis.
	The next vector that we need is a right vector that represents the positive x-axis of the camera space.
To get the right vector we use a little trick by first specifying an up vector that points upwards (in
world space). Then we do a cross product on the up vector and the direction vector from step 2.
Since the result of a cross product is a vector perpendicular to both vectors, we will get a vector that
points in the positive x-axis’s direction (if we would switch the cross product order we’d get a vector
that points in the negative x-axis).
	Now that we have both the x-axis vector and the z-axis vector, retrieving the vector that points to
the camera’s positive y-axis is relatively easy: we take the cross product of the right and direction
vector:glm::vec3 cameraUp = glm::cross(cameraDirection, cameraRight); With the help of the cross product and a few tricks we were able to create all the vectors that
form the view/camera space. For the more mathematically inclined readers, this process is known as the Gram-Schmidt process1
in linear algebra. Using these camera vectors we can now create a LookAt matrix that proves very useful for creating a camera.
	
	!Gram-Schmidt_process: en.wikipedia.org/wiki/Gram-Schmidt_process

	A great thing about matrices is that if you define a coordinate space using 3 perpendicular (or
non-linear) axes you can create a matrix with those 3 axes plus a translation vector and you can
transform any vector to that coordinate space by multiplying it with this matrix. This is exactly what
the LookAt matrix does and now that we have 3 perpendicular axes and a position vector to define
the camera space we can create our own LookAt matrix.
	Where R is the right vector, U is the up vector, D is the direction vector and P is the camera’s
position vector. Note that the rotation (left matrix) and translation (right matrix) parts are inverted
(transposed and negated respectively) since we want to rotate and translate the world in the opposite
direction of where we want the camera to move. Using this LookAt matrix as our view matrix
effectively transforms all the world coordinates to the view space we just defined. The LookAt
matrix then does exactly what it says: it creates a view matrix that looks at a given target.

	Graphics applications and games usually keep track of a deltatime variable that stores the time
it took to render the last frame. We then multiply all velocities with this deltaTime value. The
result is that when we have a large deltaTime in a frame, meaning that the last frame took longer
than average, the velocity for that frame will also be a bit higher to balance it all out. When using
this approach it does not matter if you have a very fast or slow pc, the velocity of the camera will be
balanced out accordingly so each user will have the same experience.

	Euler angles are 3 values that can represent any rotation in 3D, defined by Leonhard Euler somewhere
in the 1700s. There are 3 Euler angles: pitch, yaw and roll.
	The pitch is the angle that depicts how much we’re looking up or down as seen in the first image.
The second image shows the yaw value which represents the magnitude we’re looking to the left or
to the right. The roll represents how much we roll as mostly used in space-flight cameras. Each of
the Euler angles are represented by a single value and with the combination of all 3 of them we can
calculate any rotation vector in 3D.
	For our camera system we only care about the yaw and pitch values so we won’t discuss the roll
value here.
	A formula to convert yaw and pitch values to a 3-dimensional direction vector that
we can use for looking around:
		direction.x = cos(glm::radians(yaw)) * cos(glm::radians(pitch));
		direction.y = sin(glm::radians(pitch));
		direction.z = sin(glm::radians(yaw)) * cos(glm::radians(pitch));

	The yaw and pitch values are obtained from mouse (or controller/joystick) movement where hori￾zontal mouse-movement affects the yaw and vertical mouse-movement affects the pitch. The idea is
to store the last frame’s mouse positions and calculate in the current frame how much the mouse
values changed. The higher the horizontal or vertical difference, the more we update the pitch or
yaw value and thus the more the camera should move.

• OpenGL: a formal specification of a graphics API that defines the layout and output of each
function.
• GLAD: an extension loading library that loads and sets all OpenGL’s function pointers for us
so we can use all (modern) OpenGL’s functions.
• Viewport: the 2D window region where we render to.
• Graphics Pipeline: the entire process vertices have to walk through before ending up
as one or more pixels on the screen.
• Shader: a small program that runs on the graphics card. Several stages of the graphics
pipeline can use user-made shaders to replace existing functionality.
• Vertex: a collection of data that represent a single point.
• Normalized Device Coordinates: the coordinate system your vertices end up in
after perspective division is performed on clip coordinates. All vertex positions in NDC
between -1.0 and 1.0 will not be discarded or clipped and end up visible.
• Vertex Buffer Object: a buffer object that allocates memory on the GPU and stores
all the vertex data there for the graphics card to use.
• Vertex Array Object: stores buffer and vertex attribute state information.
• Element Buffer Object: a buffer object that stores indices on the GPU for indexed
drawing.
• Uniform: a special type of GLSL variable that is global (each shader in a shader program
can access this uniform variable) and only has to be set once.
• Texture: a special type of image used in shaders and usually wrapped around objects,
giving the illusion an object is extremely detailed.
• Texture Wrapping: defines the mode that specifies how OpenGL should sample textures
when texture coordinates are outside the range: (0, 1).
• Texture Filtering: defines the mode that specifies how OpenGL should sample the
texture when there are several texels (texture pixels) to choose from. This usually occurs
when a texture is magnified.
• Mipmaps: stored smaller versions of a texture where the appropriate sized version is chosen
based on the distance to the viewer.
• stb_image: image loading library.
• Texture Units: allows for multiple textures on a single shader program by binding
multiple textures, each to a different texture unit.
• Vector: a mathematical entity that defines directions and/or positions in any dimension.
• Matrix: a rectangular array of mathematical expressions with useful transformation proper￾ties.
• GLM: a mathematics library tailored for OpenGL.
• Local Space: the space an object begins in. All coordinates relative to an object’s origin.
• World Space: all coordinates relative to a global origin.
• View Space: all coordinates as viewed from a camera’s perspective.
• Clip Space: all coordinates as viewed from the camera’s perspective but with projection
applied. This is the space the vertex coordinates should end up in, as output of the vertex shader. OpenGL does the rest (clipping/perspective division).
• Screen Space: all coordinates as viewed from the screen. Coordinates range from 0 to
screen width/height.
• LookAt: a special type of view matrix that creates a coordinate system where all coordinates
are rotated and translated in such a way that the user is looking at a given target from a given
position.
• Euler Angles: defined as yaw, pitch and roll that allow us to form any 3D direction
vector from these 3 values

GL_TRIANGLE_FAN is a drawing mode in OpenGL that allows you to create a series of connected triangles 
sharing a common central vertex. This mode is particularly useful for drawing shapes like filled circles, polygons, and fans.



















